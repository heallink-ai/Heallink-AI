# HealLink 3D Avatar + LiveKit + Lip Sync Technical Pipeline

## ðŸŽ¯ **Complete Technical Flow - Low Level Implementation**

This document provides a comprehensive, step-by-step technical breakdown of how the HealLink 3D avatar system works from GLB loading to real-time video streaming with lip sync.

---

## **ðŸ“‹ Table of Contents**
1. [System Architecture Overview](#system-architecture-overview)
2. [3D Avatar Rendering Pipeline](#3d-avatar-rendering-pipeline)
3. [Audio Processing & Lip Sync Pipeline](#audio-processing--lip-sync-pipeline)
4. [LiveKit WebRTC Streaming Pipeline](#livekit-webrtc-streaming-pipeline)
5. [Frontend Video Display Pipeline](#frontend-video-display-pipeline)
6. [Performance & Optimization](#performance--optimization)
7. [Debugging & Troubleshooting](#debugging--troubleshooting)

---

## **ðŸ—ï¸ System Architecture Overview**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AI Agent     â”‚    â”‚  Avatar Engine   â”‚    â”‚   LiveKit      â”‚
â”‚   (FastAPI)    â”‚â”€â”€â”€â–¶â”‚   (FastAPI)     â”‚â”€â”€â”€â–¶â”‚    Server      â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ TTS Audio     â”‚    â”‚ â€¢ GLB Loading    â”‚    â”‚ â€¢ WebRTC       â”‚
â”‚ â€¢ Emotion Data  â”‚    â”‚ â€¢ 3D Rendering   â”‚    â”‚ â€¢ Video Tracks â”‚
â”‚ â€¢ Voice Sync    â”‚    â”‚ â€¢ Lip Sync       â”‚    â”‚ â€¢ Room Mgmt    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚                       â”‚
                                  â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   GLB Models     â”‚    â”‚  React Client   â”‚
                       â”‚                  â”‚    â”‚                 â”‚
                       â”‚ â€¢ doctor_avatar  â”‚    â”‚ â€¢ Video Display â”‚
                       â”‚ â€¢ Facial Shapes  â”‚    â”‚ â€¢ UI Controls   â”‚
                       â”‚ â€¢ Textures       â”‚    â”‚ â€¢ User Interact â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **ðŸŽ¨ 3D Avatar Rendering Pipeline**

### **Step 1: GLB Model Loading**
```python
# File: src/renderer/avatar_renderer.py:_load_glb_model()

1. Load GLB file using trimesh library
   â””â”€â”€ `trimesh.load("/app/assets/models/doctor_avatar_1.glb")`
   
2. Extract mesh data:
   â”œâ”€â”€ vertices: np.ndarray(120, 3) â†’ [[x,y,z], [x,y,z], ...]
   â”œâ”€â”€ faces: np.ndarray(206, 3) â†’ [[v1,v2,v3], [v1,v2,v3], ...]
   â””â”€â”€ materials: texture/color information
   
3. Normalize vertices to [-1, 1] coordinate space:
   â”œâ”€â”€ center = np.mean(vertices, axis=0)
   â”œâ”€â”€ vertices -= center  # Center at origin
   â”œâ”€â”€ max_extent = np.max(np.abs(vertices))
   â””â”€â”€ vertices /= max_extent  # Normalize to unit sphere
```

### **Step 2: Facial Blend Shapes Creation**
```python
# File: src/renderer/avatar_renderer.py:_create_facial_blend_shapes()

1. Create base expressions from vertex positions:
   â”œâ”€â”€ mouth_open: Lower face vertices moved down
   â”œâ”€â”€ smile: Mouth area vertices widened horizontally  
   â”œâ”€â”€ brow_up: Eyebrow area vertices moved up
   â””â”€â”€ eye_blink: Eye area vertices closed

2. Store as vertex offset arrays:
   â””â”€â”€ blend_shapes["mouth_open"] = deformed_vertices - base_vertices
```

### **Step 3: Real-time Frame Rendering**
```python
# File: src/renderer/avatar_renderer.py:_render_3d_mesh()

FRAME RENDERING (30 FPS):

1. Apply Blend Shapes:
   â”œâ”€â”€ Get current emotion/lip sync weights: {"mouth_open": 0.7, "smile": 0.3}
   â”œâ”€â”€ deformed_vertices = base_vertices.copy()
   â””â”€â”€ For each blend_shape: deformed_vertices += blend_shapes[shape] * weight

2. 3D to 2D Projection (Orthographic):
   â”œâ”€â”€ scale = min(width, height) * 0.6  # Dynamic scaling
   â”œâ”€â”€ center_x, center_y = width//2, height//2
   â””â”€â”€ For each vertex:
       â”œâ”€â”€ x = center_x + vertex[0] * scale
       â”œâ”€â”€ y = center_y - vertex[1] * scale  # Flip Y (screen coords)
       â””â”€â”€ projected_vertices[i] = [x, y]

3. Triangle Rasterization:
   â””â”€â”€ For each face [v1, v2, v3]:
       â”œâ”€â”€ Get 2D triangle points: pts = [projected_vertices[v1], ...]
       â”œâ”€â”€ Calculate face normal: normal = cross(edge1, edge2)
       â”œâ”€â”€ Apply lighting: intensity = max(0.4, dot(normal, light_dir))
       â”œâ”€â”€ Calculate color: face_color = base_color * intensity
       â””â”€â”€ Draw triangle: cv2.fillPoly(frame, [pts], face_color)

4. Output Frame:
   â””â”€â”€ frame: np.ndarray(1080, 1920, 3) in BGR format
```

---

## **ðŸŽ¤ Audio Processing & Lip Sync Pipeline**

### **Step 1: Audio Source (AI Agent)**
```python
# File: apps/ai-engine/agent.py

AI AGENT AUDIO OUTPUT:
1. Text-to-Speech generates audio stream
2. Audio format: 16kHz, 16-bit, mono WAV
3. Audio chunks sent to Avatar Engine via HTTP/WebSocket
4. Real-time streaming for low-latency lip sync
```

### **Step 2: Lip Sync Processing**
```python
# File: src/lipsync/lip_sync_engine.py

AUDIO â†’ VISEMES CONVERSION:

1. Audio Analysis:
   â”œâ”€â”€ librosa.load(audio_data, sr=16000)
   â”œâ”€â”€ Extract MFCC features for phoneme detection
   â””â”€â”€ Analyze amplitude for mouth opening intensity

2. Viseme Mapping:
   â”œâ”€â”€ Phoneme â†’ Viseme conversion
   â”œâ”€â”€ /p/, /b/, /m/ â†’ mouth_closed
   â”œâ”€â”€ /a/, /e/, /i/ â†’ mouth_open (various levels)
   â”œâ”€â”€ /o/, /u/ â†’ mouth_rounded
   â””â”€â”€ /s/, /z/ â†’ mouth_narrow

3. Temporal Smoothing:
   â”œâ”€â”€ Apply low-pass filter to prevent jittery animation
   â”œâ”€â”€ Interpolate between viseme states
   â””â”€â”€ Generate smooth weight transitions

4. Output Blend Shape Weights:
   â””â”€â”€ {"mouth_open": 0.7, "mouth_rounded": 0.3, "mouth_smile": 0.0}
```

### **Step 3: Real-time Sync**
```python
# File: src/plugin/avatar_session.py:_process_audio_loop()

AUDIO-VISUAL SYNCHRONIZATION:

1. Audio Buffer Management:
   â”œâ”€â”€ Incoming audio â†’ asyncio.Queue(maxsize=1000)
   â”œâ”€â”€ Process chunks in 10ms intervals
   â””â”€â”€ Maintain 50-100ms buffer for smooth playback

2. Lip Sync Application:
   â”œâ”€â”€ Process audio chunk â†’ lip_sync_data
   â”œâ”€â”€ Update renderer: await renderer.update_mouth_animation(lip_sync_data)
   â”œâ”€â”€ Apply blend shapes in next frame render
   â””â”€â”€ Synchronize with 30fps video output (33.3ms intervals)
```

---

## **ðŸ“¡ LiveKit WebRTC Streaming Pipeline**

### **Step 1: Avatar Engine â†’ LiveKit Connection**
```python
# File: src/plugin/avatar_session.py:start_livekit_streaming()

LIVEKIT CONNECTION SETUP:

1. Room Connection:
   â”œâ”€â”€ room = rtc.Room()
   â”œâ”€â”€ await room.connect(livekit_url, livekit_token)
   â””â”€â”€ participant_identity = "Heallink Health Assistant"

2. Video Track Creation:
   â”œâ”€â”€ video_source = rtc.VideoSource(1920, 1080)
   â”œâ”€â”€ video_track = rtc.LocalVideoTrack.create_video_track("avatar-video", video_source)
   â””â”€â”€ publish_options = rtc.TrackPublishOptions(source=SOURCE_CAMERA)

3. Track Publishing:
   â””â”€â”€ await room.local_participant.publish_track(video_track, publish_options)
```

### **Step 2: Real-time Frame Streaming**
```python
# File: src/plugin/avatar_session.py:_render_loop()

VIDEO FRAME PIPELINE (30 FPS):

1. Frame Generation:
   â”œâ”€â”€ frame = await renderer.render_frame()  # (1080, 1920, 3) BGR
   â”œâ”€â”€ Validates frame is not None
   â””â”€â”€ Frame contains 3D avatar + background

2. Color Space Conversion:
   â”œâ”€â”€ frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
   â””â”€â”€ Required: LiveKit expects RGB, OpenCV generates BGR

3. LiveKit VideoFrame Creation:
   â”œâ”€â”€ video_frame = rtc.VideoFrame(
   â”‚     width=frame_rgb.shape[1],      # 1920
   â”‚     height=frame_rgb.shape[0],     # 1080  
   â”‚     type=rtc.VideoBufferType.RGB24,
   â”‚     data=frame_rgb.tobytes()
   â”‚   )
   â””â”€â”€ Raw pixel data as contiguous byte array

4. Frame Publishing:
   â”œâ”€â”€ video_source.capture_frame(video_frame)
   â”œâ”€â”€ LiveKit encodes frame (VP8/H.264)
   â”œâ”€â”€ Sends via WebRTC to connected clients
   â””â”€â”€ Maintains 30fps target (33.3ms intervals)
```

### **Step 3: WebRTC Transport**
```
LIVEKIT WEBRTC FLOW:

Avatar Engine â†’ LiveKit Server â†’ React Client

1. Avatar Engine:
   â”œâ”€â”€ Generates RGB frames at 30fps
   â”œâ”€â”€ Publishes to video_source.capture_frame()
   â””â”€â”€ LocalVideoTrack streams to LiveKit

2. LiveKit Server:
   â”œâ”€â”€ Receives raw frames from Avatar Engine
   â”œâ”€â”€ Encodes with VP8/H.264 codec
   â”œâ”€â”€ Manages WebRTC peer connections
   â”œâ”€â”€ Handles network adaptation/quality
   â””â”€â”€ Routes video to subscribed participants

3. Network Transport:
   â”œâ”€â”€ UDP/SRTP for low latency
   â”œâ”€â”€ ICE for NAT traversal
   â”œâ”€â”€ DTLS for encryption
   â””â”€â”€ Adaptive bitrate based on network conditions
```

---

## **ðŸ–¥ï¸ Frontend Video Display Pipeline**

### **Step 1: LiveKit Client Connection**
```typescript
// File: app/providers/LiveKitProvider.tsx

REACT LIVEKIT INTEGRATION:

1. Room Connection:
   â”œâ”€â”€ room = new Room()
   â”œâ”€â”€ await room.connect(wsUrl, token)
   â””â”€â”€ Auto-reconnection and error handling

2. Participant Discovery:
   â”œâ”€â”€ Listen for "participant_connected" events
   â”œâ”€â”€ Filter by participant.identity === "Heallink Health Assistant"
   â””â”€â”€ Track avatar participant separately from user participants
```

### **Step 2: Video Track Subscription**
```typescript
// File: app/features/dashboard/components/VoiceAgentAvatar.tsx

VIDEO TRACK ATTACHMENT:

1. Track Discovery:
   â”œâ”€â”€ participant.trackPublications.values()
   â”œâ”€â”€ Filter: publication.kind === Track.Kind.Video
   â”œâ”€â”€ Check: publication.isSubscribed && publication.track
   â””â”€â”€ Handle async track subscription

2. Video Element Attachment:
   â”œâ”€â”€ track.attach(videoElement)  # LiveKit handles MediaStream creation
   â”œâ”€â”€ videoElement.autoPlay = true
   â”œâ”€â”€ videoElement.playsInline = true
   â””â”€â”€ CSS styling for proper display

3. Error Handling:
   â”œâ”€â”€ Retry mechanism for failed attachments
   â”œâ”€â”€ Fallback to placeholder on track errors
   â”œâ”€â”€ Debug logging for troubleshooting
   â””â”€â”€ Graceful degradation
```

### **Step 3: Frontend Rendering**
```css
/* File: app/styles/components/voice-agent-avatar.css */

VIDEO DISPLAY STYLING:

.avatar-video-container {
  width: 240px;
  height: 240px;
  border-radius: 16px;    /* Rounded rectangle (not circle) */
  overflow: hidden;       /* Clip content to container */
}

.avatar-video {
  width: 100%;
  height: 100%;
  object-fit: contain;    /* Show full avatar (not cropped) */
  background: transparent;
  border-radius: 16px;
}
```

---

## **âš¡ Performance & Optimization**

### **Frame Rate Optimization**
```python
# Target: 30 FPS (33.3ms per frame)

RENDERING PERFORMANCE:
â”œâ”€â”€ 3D Mesh: ~5-10ms (120 vertices, 206 faces)
â”œâ”€â”€ Lighting: ~2-3ms (simple directional lighting)
â”œâ”€â”€ Blend Shapes: ~1-2ms (4 active shapes)
â”œâ”€â”€ Background: ~1ms (solid color fill)
â”œâ”€â”€ Color Conversion: ~1ms (BGRâ†’RGB)
â””â”€â”€ LiveKit Publish: ~2-5ms
   
Total: ~12-22ms per frame (well under 33.3ms budget)
```

### **Memory Management**
```python
MEMORY EFFICIENCY:
â”œâ”€â”€ Vertex arrays: 120 Ã— 3 Ã— 4 bytes = 1.4KB
â”œâ”€â”€ Face arrays: 206 Ã— 3 Ã— 4 bytes = 2.4KB  
â”œâ”€â”€ Frame buffer: 1920 Ã— 1080 Ã— 3 = 6.2MB
â”œâ”€â”€ Blend shapes: 4 Ã— 1.4KB = 5.6KB
â””â”€â”€ Total per avatar: ~6.5MB (very efficient)
```

---

## **ðŸ› Debugging & Troubleshooting**

### **Common Issues & Solutions**

#### **1. Avatar Not Visible (Brown Background)**
```bash
# Enable debug mode
export DEBUG=true
export DEBUG_WIREFRAME=true
export DEBUG_VERTICES=true

# Run debug test
docker exec avatar-engine python test_avatar_debug.py

# Check debug files:
# - debug_avatar_frame.png (full rendering)
# - debug_faces.png (individual triangles) 
# - debug_vertices.png (vertex positions)
```

#### **2. LiveKit Connection Issues**
```bash
# Check LiveKit logs
docker logs heallink-avatar-engine | grep -i livekit

# Verify room connection
curl -X GET "http://localhost:7880/rooms"

# Check video track publishing
# Should see: "Published avatar video track: avatar_doctor_avatar_1"
```

#### **3. Lip Sync Delay**
```python
# Reduce audio buffer size
self.audio_buffer = asyncio.Queue(maxsize=100)  # Smaller buffer

# Increase processing frequency  
await asyncio.sleep(0.005)  # 5ms instead of 10ms

# Check audio latency in logs
logger.info(f"Audio processing delay: {current_time - audio_timestamp}ms")
```

#### **4. Frontend Video Issues**
```typescript
// Check video track status
console.log("Video tracks:", participant.videoTrackPublications);
console.log("Has video:", hasVideoTrack);
console.log("Video element:", videoRef.current);

// Verify track attachment
videoElement.onloadeddata = () => console.log("Video loaded");
videoElement.onplay = () => console.log("Video playing");
```

---

## **ðŸŽ¯ Summary: End-to-End Data Flow**

```
1. AI Agent generates TTS audio
2. Audio â†’ Avatar Engine via HTTP/WebSocket  
3. Lip Sync Engine: Audio â†’ Viseme weights
4. 3D Renderer: GLB + Blend Shapes â†’ Video frame
5. LiveKit: RGB frame â†’ WebRTC encoding
6. Network: VP8/H.264 stream â†’ React client
7. Frontend: Video track â†’ HTML video element
8. User sees: Real-time 3D avatar with lip sync
```

**Total Latency: ~50-100ms** (Speech â†’ Visible lip movement)

---

## **ðŸš€ Next Steps for Optimization**

1. **GPU Acceleration**: Use OpenGL/WebGL for 3D rendering
2. **Advanced Lip Sync**: Train custom phoneme-to-viseme models
3. **Eye Tracking**: Add eye movement and blinking
4. **Texture Mapping**: Apply realistic skin textures
5. **Performance**: SIMD optimizations for vertex processing
6. **Quality**: Anti-aliasing and better lighting models

---

*This documentation covers the complete technical pipeline from GLB loading to real-time video streaming. Each component is optimized for low latency and high performance in healthcare applications.*